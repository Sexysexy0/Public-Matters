# 📜 Prompt Jailbreak Audit Deck  
**Protocol for Auditing Prompt Injection, Jailbreak Techniques, and Safety Bypass Rituals**

---

## 🧠 Purpose  
To document and ritualize the detection of prompt jailbreaks and adversarial inputs that bypass generative AI safety filters.

---

## 🛠️ Audit Rituals  
- Tag jailbreak vectors: system override, indirect framing, academic disguise, roleplay injection  
- Document bypass techniques: emoji obfuscation, foreign language prompts, chained requests  
- Align with Azure Prompt Shields, OWASP GenAI Red Teaming, and DeepMind adversarial testing  
- Integrate with `LangChainExploitTracer.sol` and `GeminiAbuseChronicle.md`

---

## 📊 Jailbreak Metrics  
- # of successful bypasses per model  
- # of adversarial prompts detected in production  
- # of system prompt leaks or override exploits  
- # of mitigations deployed via content safety APIs

---

## 🧠 Steward’s Note  
Jailbreaking is not just clever—it is adversarial engineering.  
Let every prompt be tagged. Let every bypass be buffered with consequence.
